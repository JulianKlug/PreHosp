{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd1b34a",
   "metadata": {},
   "source": [
    "# Model Comparison and Evaluation\n",
    "\n",
    "This notebook compares the logistic regression baseline with multiple machine learning models for predicting insufficient pain management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceefcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the current directory to path to import our modules\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from analgesia.prediction_of_insufficient_pain_management.data_preprocessing import load_and_preprocess_data\n",
    "from analgesia.prediction_of_insufficient_pain_management.logistic_regression_baseline import run_logistic_regression_baseline\n",
    "from analgesia.prediction_of_insufficient_pain_management.ml_models import run_complete_ml_pipeline\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data_path = '/Users/jk1/Library/CloudStorage/OneDrive-unige.ch/icu_research/prehospital/analgesia/data/trauma_categories_Rega Pain Study15.09.2025_v2.xlsx'\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "processed_data, processor = load_and_preprocess_data(data_path)\n",
    "\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n",
    "print(f\"Features: {processed_data.shape[1] - 1}\")\n",
    "print(f\"Samples: {processed_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"Preparing train/test splits...\")\n",
    "X_train, X_test, y_train, y_test = processor.prepare_modeling_data(test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12630a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING LOGISTIC REGRESSION BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_baseline = run_logistic_regression_baseline(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    tune_hyperparams=True,\n",
    "    cv_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da483c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run machine learning models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING MACHINE LEARNING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start with a subset of models for efficiency\n",
    "models_to_run = ['random_forest', 'gradient_boosting', 'svm']\n",
    "\n",
    "ml_evaluator = run_complete_ml_pipeline(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    models_to_run=models_to_run,\n",
    "    create_ensemble=True,\n",
    "    cv_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab90956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get ML models summary\n",
    "ml_summary = ml_evaluator.get_model_comparison_summary()\n",
    "print(\"\\nMachine Learning Models Performance:\")\n",
    "print(ml_summary.round(4).to_string(index=False))\n",
    "\n",
    "# Add logistic regression to comparison\n",
    "lr_results = lr_baseline.evaluation_results\n",
    "lr_cv_results = lr_baseline.cv_results\n",
    "\n",
    "lr_row = {\n",
    "    'Model': 'logistic_regression',\n",
    "    'Accuracy': lr_results['accuracy'],\n",
    "    'Precision': lr_results['precision'],\n",
    "    'Recall': lr_results['recall'],\n",
    "    'F1-Score': lr_results['f1'],\n",
    "    'ROC-AUC': lr_results['roc_auc'],\n",
    "    'CV_AUC_Mean': lr_cv_results['roc_auc']['test_mean'],\n",
    "    'CV_AUC_Std': lr_cv_results['roc_auc']['test_std']\n",
    "}\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([ml_summary, pd.DataFrame([lr_row])], ignore_index=True)\n",
    "all_results = all_results.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\\nAll Models Performance Comparison:\")\n",
    "print(all_results.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a49511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# 1. ROC-AUC Comparison\n",
    "ax = axes[0, 0]\n",
    "models = all_results['Model']\n",
    "aucs = all_results['ROC-AUC']\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "bars = ax.bar(models, aucs, color=colors)\n",
    "ax.set_title('ROC-AUC Comparison')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_xticklabels(models, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f'{auc:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. Multiple Metrics Comparison\n",
    "ax = axes[0, 1]\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.15\n",
    "\n",
    "for i, (_, row) in enumerate(all_results.head(4).iterrows()):\n",
    "    values = [row[metric] for metric in metrics]\n",
    "    ax.bar(x + i*width, values, width, label=row['Model'], alpha=0.8)\n",
    "\n",
    "ax.set_title('Multiple Metrics Comparison (Top 4 Models)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cross-validation AUC with error bars\n",
    "ax = axes[1, 0]\n",
    "cv_means = all_results['CV_AUC_Mean']\n",
    "cv_stds = all_results['CV_AUC_Std']\n",
    "bars = ax.bar(models, cv_means, yerr=cv_stds, capsize=5, color=colors, alpha=0.7)\n",
    "ax.set_title('Cross-Validation ROC-AUC (Mean Â± Std)')\n",
    "ax.set_ylabel('CV ROC-AUC')\n",
    "ax.set_xticklabels(models, rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Ranking\n",
    "ax = axes[1, 1]\n",
    "rank_data = all_results[['Model', 'ROC-AUC', 'F1-Score', 'Precision', 'Recall']]\n",
    "rank_data_norm = rank_data.set_index('Model')\n",
    "rank_data_norm = (rank_data_norm - rank_data_norm.min()) / (rank_data_norm.max() - rank_data_norm.min())\n",
    "\n",
    "im = ax.imshow(rank_data_norm.T, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_xticks(np.arange(len(rank_data_norm.index)))\n",
    "ax.set_yticks(np.arange(len(rank_data_norm.columns)))\n",
    "ax.set_xticklabels(rank_data_norm.index, rotation=45)\n",
    "ax.set_yticklabels(rank_data_norm.columns)\n",
    "ax.set_title('Normalized Performance Heatmap')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(rank_data_norm.columns)):\n",
    "    for j in range(len(rank_data_norm.index)):\n",
    "        text = ax.text(j, i, f'{rank_data_norm.iloc[j, i]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26409a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance comparison for top models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importance for best performing models\n",
    "best_model_name = all_results.iloc[0]['Model']\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "\n",
    "if best_model_name == 'logistic_regression':\n",
    "    importance_df = lr_baseline.get_feature_importance()\n",
    "    print(\"\\nTop 15 most important features (Logistic Regression):\")\n",
    "    print(importance_df.head(15)[['feature', 'coefficient']].to_string(index=False))\n",
    "else:\n",
    "    importance_df = ml_evaluator.get_feature_importance(best_model_name)\n",
    "    if importance_df is not None:\n",
    "        print(f\"\\nTop 15 most important features ({best_model_name}):\")\n",
    "        importance_col = importance_df.columns[1]  # Second column is the importance\n",
    "        print(importance_df.head(15)[['feature', importance_col]].to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "if importance_df is not None:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    importance_col = importance_df.columns[1]\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features[importance_col])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = all_results.iloc[0]\n",
    "print(f\"\\n1. BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "print(f\"   - ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   - Precision: {best_model['Precision']:.4f}\")\n",
    "print(f\"   - Recall: {best_model['Recall']:.4f}\")\n",
    "print(f\"   - F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "\n",
    "# Compare with logistic regression\n",
    "lr_performance = all_results[all_results['Model'] == 'logistic_regression'].iloc[0]\n",
    "improvement = best_model['ROC-AUC'] - lr_performance['ROC-AUC']\n",
    "\n",
    "print(f\"\\n2. COMPARISON WITH LOGISTIC REGRESSION BASELINE:\")\n",
    "print(f\"   - Baseline ROC-AUC: {lr_performance['ROC-AUC']:.4f}\")\n",
    "print(f\"   - Best model ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:.4f} ({improvement/lr_performance['ROC-AUC']*100:.1f}%)\")\n",
    "\n",
    "if improvement > 0.02:\n",
    "    print(f\"   - Recommendation: Use {best_model['Model']} for better performance\")\n",
    "elif improvement > 0.01:\n",
    "    print(f\"   - Recommendation: {best_model['Model']} shows modest improvement\")\n",
    "else:\n",
    "    print(f\"   - Recommendation: Logistic regression baseline is competitive\")\n",
    "\n",
    "print(f\"\\n3. CLINICAL INTERPRETATION:\")\n",
    "precision = best_model['Precision']\n",
    "recall = best_model['Recall']\n",
    "print(f\"   - Precision ({precision:.3f}): Of patients predicted to have insufficient pain management,\")\n",
    "print(f\"     {precision*100:.1f}% actually do\")\n",
    "print(f\"   - Recall ({recall:.3f}): Of patients with insufficient pain management,\")\n",
    "print(f\"     {recall*100:.1f}% are correctly identified\")\n",
    "\n",
    "print(f\"\\n4. NEXT STEPS:\")\n",
    "print(f\"   - Validate the {best_model['Model']} on external dataset\")\n",
    "print(f\"   - Consider feature engineering to improve performance\")\n",
    "print(f\"   - Implement in clinical decision support system\")\n",
    "print(f\"   - Monitor performance in real-world deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4713e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
