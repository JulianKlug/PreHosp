{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37fc53a",
   "metadata": {},
   "source": [
    "# Model Comparison: XGBoost vs Logistic Regression\n",
    "\n",
    "This notebook creates a comprehensive comparison figure between the best performing XGBoost model and baseline Logistic Regression for insufficient pain management prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the parent directory to the path\n",
    "sys.path.append('/Users/jk1/icu_research/PreHosp')\n",
    "\n",
    "from analgesia.prediction_of_insufficient_pain_management.data_preprocessing import load_and_preprocess_data\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2dcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "print(\"📊 Loading and preprocessing data for model comparison...\")\n",
    "data_path = '/Users/jk1/Library/CloudStorage/OneDrive-unige.ch/icu_research/prehospital/analgesia/data/trauma_categories_Rega Pain Study15.09.2025_v2.xlsx'\n",
    "processed_data, processor = load_and_preprocess_data(data_path)\n",
    "\n",
    "# Get the modeling data splits\n",
    "X_train, X_test, y_train, y_test = processor.prepare_modeling_data()\n",
    "\n",
    "print(f\"✅ Data loaded:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"   Target balance - Training: {y_train.mean():.1%}, Test: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a17737",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model (using parameters from previous testing)\n",
    "print(\"🚀 Training XGBoost model...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✅ XGBoost model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "print(\"📈 Training Logistic Regression model...\")\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✅ Logistic Regression model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36560cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics for both models\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba, model_name):\n",
    "    \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1-Score': f1_score(y_true, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_true, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Calculate PR-AUC\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    metrics['PR-AUC'] = auc(recall_vals, precision_vals)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for both models\n",
    "xgb_metrics = calculate_metrics(y_test, xgb_pred, xgb_pred_proba, 'XGBoost')\n",
    "lr_metrics = calculate_metrics(y_test, lr_pred, lr_pred_proba, 'Logistic Regression')\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([xgb_metrics, lr_metrics])\n",
    "\n",
    "print(\"📊 Model Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC']:\n",
    "    print(f\"{col:12} | XGBoost: {xgb_metrics[col]:.3f} | LogReg: {lr_metrics[col]:.3f}\")\n",
    "    \n",
    "print(\"\\n✅ Performance metrics calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940340d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison figure\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Define colors for consistency\n",
    "xgb_color = '#2E86AB'  # Blue\n",
    "lr_color = '#A23B72'   # Magenta\n",
    "\n",
    "# 1. ROC Curves (top left)\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "# XGBoost ROC\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_pred_proba)\n",
    "plt.plot(fpr_xgb, tpr_xgb, color=xgb_color, linewidth=2, \n",
    "         label=f'XGBoost (AUC = {xgb_metrics[\"ROC-AUC\"]:.3f})')\n",
    "\n",
    "# Logistic Regression ROC\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_pred_proba)\n",
    "plt.plot(fpr_lr, tpr_lr, color=lr_color, linewidth=2,\n",
    "         label=f'Logistic Regression (AUC = {lr_metrics[\"ROC-AUC\"]:.3f})')\n",
    "\n",
    "# Reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curves (top middle)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "# XGBoost PR\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, xgb_pred_proba)\n",
    "plt.plot(recall_xgb, precision_xgb, color=xgb_color, linewidth=2,\n",
    "         label=f'XGBoost (AUC = {xgb_metrics[\"PR-AUC\"]:.3f})')\n",
    "\n",
    "# Logistic Regression PR\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, lr_pred_proba)\n",
    "plt.plot(recall_lr, precision_lr, color=lr_color, linewidth=2,\n",
    "         label=f'Logistic Regression (AUC = {lr_metrics[\"PR-AUC\"]:.3f})')\n",
    "\n",
    "# Baseline (proportion of positive class)\n",
    "baseline = y_test.mean()\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', alpha=0.5, \n",
    "           label=f'Baseline ({baseline:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Performance Metrics Bar Chart (top right)\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x_pos = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "xgb_values = [xgb_metrics[m] for m in metrics_to_plot]\n",
    "lr_values = [lr_metrics[m] for m in metrics_to_plot]\n",
    "\n",
    "bars1 = plt.bar(x_pos - width/2, xgb_values, width, label='XGBoost', \n",
    "                color=xgb_color, alpha=0.8)\n",
    "bars2 = plt.bar(x_pos + width/2, lr_values, width, label='Logistic Regression',\n",
    "                color=lr_color, alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    plt.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.01,\n",
    "             f'{xgb_values[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    plt.text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.01,\n",
    "             f'{lr_values[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(x_pos, metrics_to_plot, rotation=45)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Confusion Matrices (bottom left and middle)\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "cm_xgb = confusion_matrix(y_test, xgb_pred)\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Adequate', 'Insufficient'],\n",
    "            yticklabels=['Adequate', 'Insufficient'])\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "cm_lr = confusion_matrix(y_test, lr_pred)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Reds',\n",
    "            xticklabels=['Adequate', 'Insufficient'],\n",
    "            yticklabels=['Adequate', 'Insufficient'])\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# 6. Model Summary Table (bottom right)\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('tight')\n",
    "ax6.axis('off')\n",
    "\n",
    "# Create summary table\n",
    "summary_data = [\n",
    "    ['Metric', 'XGBoost', 'Logistic Regression', 'Difference'],\n",
    "    ['Accuracy', f'{xgb_metrics[\"Accuracy\"]:.3f}', f'{lr_metrics[\"Accuracy\"]:.3f}', \n",
    "     f'{xgb_metrics[\"Accuracy\"] - lr_metrics[\"Accuracy\"]:+.3f}'],\n",
    "    ['ROC-AUC', f'{xgb_metrics[\"ROC-AUC\"]:.3f}', f'{lr_metrics[\"ROC-AUC\"]:.3f}',\n",
    "     f'{xgb_metrics[\"ROC-AUC\"] - lr_metrics[\"ROC-AUC\"]:+.3f}'],\n",
    "    ['PR-AUC', f'{xgb_metrics[\"PR-AUC\"]:.3f}', f'{lr_metrics[\"PR-AUC\"]:.3f}',\n",
    "     f'{xgb_metrics[\"PR-AUC\"] - lr_metrics[\"PR-AUC\"]:+.3f}'],\n",
    "    ['F1-Score', f'{xgb_metrics[\"F1-Score\"]:.3f}', f'{lr_metrics[\"F1-Score\"]:.3f}',\n",
    "     f'{xgb_metrics[\"F1-Score\"] - lr_metrics[\"F1-Score\"]:+.3f}'],\n",
    "    ['Precision', f'{xgb_metrics[\"Precision\"]:.3f}', f'{lr_metrics[\"Precision\"]:.3f}',\n",
    "     f'{xgb_metrics[\"Precision\"] - lr_metrics[\"Precision\"]:+.3f}'],\n",
    "    ['Recall', f'{xgb_metrics[\"Recall\"]:.3f}', f'{lr_metrics[\"Recall\"]:.3f}',\n",
    "     f'{xgb_metrics[\"Recall\"] - lr_metrics[\"Recall\"]:+.3f}']\n",
    "]\n",
    "\n",
    "table = ax6.table(cellText=summary_data[1:], colLabels=summary_data[0],\n",
    "                  cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "# Color code the difference column\n",
    "for i in range(1, len(summary_data)):\n",
    "    diff_val = float(summary_data[i][3])\n",
    "    if diff_val > 0:\n",
    "        table[(i, 3)].set_facecolor('#E8F5E8')  # Light green for positive\n",
    "    else:\n",
    "        table[(i, 3)].set_facecolor('#FFE8E8')  # Light red for negative\n",
    "\n",
    "plt.title('Performance Summary', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Comprehensive model comparison figure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27519bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "output_path = \"/Users/jk1/icu_research/PreHosp/analgesia/prediction_of_insufficient_pain_management/model_comparison_xgboost_vs_logistic.png\"\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"💾 Figure saved to: {output_path}\")\n",
    "\n",
    "# Also save as PDF for publication\n",
    "pdf_path = output_path.replace('.png', '.pdf')\n",
    "plt.savefig(pdf_path, bbox_inches='tight', facecolor='white')\n",
    "print(f\"📄 PDF version saved to: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed comparison summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 DETAILED MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 Dataset Information:\")\n",
    "print(f\"   • Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   • Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"   • Features: {X_train.shape[1]}\")\n",
    "print(f\"   • Insufficient pain management rate: {y_test.mean():.1%}\")\n",
    "\n",
    "print(\"\\n🔍 Key Performance Differences:\")\n",
    "print(f\"   • Accuracy improvement: {(xgb_metrics['Accuracy'] - lr_metrics['Accuracy'])*100:+.1f} percentage points\")\n",
    "print(f\"   • ROC-AUC improvement: {(xgb_metrics['ROC-AUC'] - lr_metrics['ROC-AUC'])*100:+.1f} percentage points\")\n",
    "print(f\"   • PR-AUC improvement: {(xgb_metrics['PR-AUC'] - lr_metrics['PR-AUC'])*100:+.1f} percentage points\")\n",
    "print(f\"   • F1-Score improvement: {(xgb_metrics['F1-Score'] - lr_metrics['F1-Score'])*100:+.1f} percentage points\")\n",
    "\n",
    "print(\"\\n⚖️ Trade-offs Analysis:\")\n",
    "print(f\"   • XGBoost Precision: {xgb_metrics['Precision']:.3f} vs LogReg: {lr_metrics['Precision']:.3f}\")\n",
    "print(f\"   • XGBoost Recall: {xgb_metrics['Recall']:.3f} vs LogReg: {lr_metrics['Recall']:.3f}\")\n",
    "\n",
    "# Determine which model is better overall\n",
    "xgb_better = sum([\n",
    "    xgb_metrics['Accuracy'] > lr_metrics['Accuracy'],\n",
    "    xgb_metrics['ROC-AUC'] > lr_metrics['ROC-AUC'],\n",
    "    xgb_metrics['PR-AUC'] > lr_metrics['PR-AUC'],\n",
    "    xgb_metrics['F1-Score'] > lr_metrics['F1-Score']\n",
    "])\n",
    "\n",
    "print(f\"\\n🏅 Overall Winner: {'XGBoost' if xgb_better >= 3 else 'Logistic Regression'}\")\n",
    "print(f\"   • XGBoost wins in {xgb_better}/4 key metrics\")\n",
    "\n",
    "print(\"\\n📈 Clinical Interpretation:\")\n",
    "if xgb_metrics['ROC-AUC'] > lr_metrics['ROC-AUC']:\n",
    "    print(f\"   • XGBoost shows better discriminative ability (ROC-AUC: {xgb_metrics['ROC-AUC']:.3f})\")\n",
    "if xgb_metrics['PR-AUC'] > lr_metrics['PR-AUC']:\n",
    "    print(f\"   • XGBoost is better at identifying insufficient pain management cases\")\n",
    "if xgb_metrics['Recall'] > lr_metrics['Recall']:\n",
    "    print(f\"   • XGBoost catches more true insufficient pain management cases\")\n",
    "else:\n",
    "    print(f\"   • Logistic Regression has higher sensitivity ({lr_metrics['Recall']:.3f})\")\n",
    "\n",
    "print(\"\\n✅ Model comparison analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed metrics to CSV for further analysis\n",
    "comparison_df_detailed = pd.DataFrame([xgb_metrics, lr_metrics])\n",
    "csv_path = \"/Users/jk1/icu_research/PreHosp/analgesia/prediction_of_insufficient_pain_management/model_comparison_metrics.csv\"\n",
    "comparison_df_detailed.to_csv(csv_path, index=False)\n",
    "print(f\"💾 Detailed metrics saved to: {csv_path}\")\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"\\n📋 DETAILED METRICS TABLE:\")\n",
    "print(\"=\" * 50)\n",
    "display(comparison_df_detailed.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
